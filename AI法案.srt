1
00:00:00,000 --> 00:00:12,320
今回のテーマはAI法案

2
00:00:12,320 --> 00:00:17,820
AIに対処するですね法律の案というのが出されまして

3
00:00:17,820 --> 00:00:22,400
2025年のね今国会で成立を目指しているということなんですけれども

4
00:00:22,400 --> 00:00:25,840
はいまずAIって何なのかっていうのは

5
00:00:25,840 --> 00:00:31,260
これは説明しなくても皆さんねもういいよっていう人も多いかもしれませんね人工知能ですねAI

6
00:00:31,260 --> 00:00:39,900
ただそのAIの本質って何なのかっていうとまあ要はですね大量にデータを学習できるこれに尽きるんですよね

7
00:00:39,900 --> 00:00:47,000
人間って例えば読める本の数ってまあ1日にどうですか1冊読めたらすごいですよね

8
00:00:47,000 --> 00:00:52,020
これを1億冊読めるみたいなのがこのAIなんですよね

9
00:00:52,020 --> 00:00:58,640
だからもう人間では全くできないような量のデータを学習するんですね

10
00:00:58,640 --> 00:01:05,160
そうすると何ができるようになるかっていうとさまざまな分析推測これができるようになると

11
00:01:05,160 --> 00:01:10,200
どういうことかと言いますとですね例えばですよ本当に例えばなんですけれども

12
00:01:10,200 --> 00:01:18,380
土曜日の夜7時に新宿駅から渋谷駅に電車に乗る人がいるとしますよね

13
00:01:18,380 --> 00:01:20,220
そういう人は多分たくさんいるんですけれども

14
00:01:20,220 --> 00:01:27,340
その人が渋谷の次に向かう駅はどこかみたいなのもデータをいっぱい抽出できるわけですよ

15
00:01:27,340 --> 00:01:33,760
だって土曜日だってね1年に何回ですか50何回とかあるんでしょうしそれが何年もあるわけですもんね

16
00:01:33,760 --> 00:01:41,760
そうすると一定の行動を取った人っていうのが次に何をするかの推測分析なんかができるようになると

17
00:01:41,760 --> 00:01:45,420
量がたくさんあるとこういうことができるようになるんですよね

18
00:01:45,420 --> 00:01:48,440
例えば医療なんかでも応用が期待できます

19
00:01:48,440 --> 00:01:55,520
つまりこういう病気になった人っていうのが過去にどんなことをして

20
00:01:55,520 --> 00:01:59,660
どんなお薬を飲んでそうしたらこうなりましたっていう例がいっぱいあるわけですよね

21
00:01:59,660 --> 00:02:06,140
そうするとその中からこの病気に対する対策としてはこういうねやり方が一番いいんじゃないか

22
00:02:06,140 --> 00:02:12,520
こういう手術をするのがいいんじゃないか手術をするにしてもねするかしないかあるいはいつするのか

23
00:02:12,520 --> 00:02:19,520
あるいは薬もねどの薬を飲んだらいいのかなんていうことも今までの症例からですね推測をすることができる

24
00:02:19,520 --> 00:02:25,440
法律にも使えそうですよね過去にこういう判例がありましたみたいなこともですねすごくわかりやすくなると

25
00:02:25,440 --> 00:02:32,180
ただ悪用も当然できるわけです悪いことにも使えるすでに使われています

26
00:02:32,180 --> 00:02:36,520
例えばディープフェイクって言われるものなんですよね

27
00:02:36,520 --> 00:02:43,300
これは何かと言いますと本物と見分けがつかない見せ物のことです

28
00:02:43,300 --> 00:02:45,000
これをディープフェイクって言います

29
00:02:45,000 --> 00:02:50,340
画像とか音声である場合が多いですね映像ってこともありますね

30
00:02:50,340 --> 00:02:52,620
もうこれ見たことあるっていう人もいるかもしれません

31
00:02:52,620 --> 00:03:00,280
誰もが知っているようなね著名人有名な人がまるでなんかその人が話しているような感じで

32
00:03:00,280 --> 00:03:02,940
でもその人が本当は言っていないことを喋っている

33
00:03:02,940 --> 00:03:09,380
ね怪しげなこの株を買ったらいいよみたいな投資のね情報をなんか喋っているように見せかける

34
00:03:09,380 --> 00:03:13,860
なんてことがもうすでにねSNSなんかで見た人もいるかもしれませんけれども

35
00:03:13,860 --> 00:03:22,320
一番困るものは何か例えば選挙ですよね選挙ってのはとても大事な政治にとってね

36
00:03:22,320 --> 00:03:24,860
国の根幹と言っても過言じゃないでしょう

37
00:03:24,860 --> 00:03:30,140
どの政治家に国を代表させるかというのを決めるこれがね選挙ですけれども

38
00:03:30,140 --> 00:03:37,260
選挙期間中にですね立候補している候補者の偽の音声

39
00:03:37,260 --> 00:03:42,400
これをディープフェイクで作ってですね拡散するこれ実際にありました

40
00:03:42,400 --> 00:03:46,000
2023年スロバキアっていう国がねヨーロッパにあるんですけれども

41
00:03:46,000 --> 00:03:51,960
その総選挙でですね投票日の数日前にこの偽音声が拡散したと

42
00:03:51,960 --> 00:03:57,400
あるいはですね台湾で総統選という選挙があったんですけれども

43
00:03:57,400 --> 00:04:06,440
2024年去年あったんですがそこではですね中国の習近平国家主席の偽物と見られる動画が広がったと

44
00:04:06,440 --> 00:04:12,420
こういうことってのはどんどん増えているし成功さを増している

45
00:04:12,420 --> 00:04:14,400
もう見分けがつかなくなりつつあるんですね

46
00:04:14,400 --> 00:04:20,960
そうするとですね習近平さんがこう言ってんだったらこの人に投票しようとかするのをやめようとか

47
00:04:20,960 --> 00:04:26,100
ねあるいはさっきのスロバキアの話だとこの人こんなこと言う人なんだ投票やめようとか

48
00:04:26,100 --> 00:04:29,200
こんないいことを言うんだったらぜひ投票しようというか

49
00:04:29,200 --> 00:04:32,660
まあ多分ね投票するのをやめさせるって方が多いんじゃないかなと思いますけれども

50
00:04:32,660 --> 00:04:40,360
そういうことがあると困りますよね選挙っていうのがねそうやって操縦されてしまうっていうことにもなりかねない

51
00:04:40,360 --> 00:04:48,960
これもだからAIのこういうねいろんなデータを学習できる対応に学習できるってところが悪用されているわけです

52
00:04:48,960 --> 00:04:54,340
つまりその誰かがその特定の人がですね本当に言いそうなこと

53
00:04:54,340 --> 00:04:59,200
次に言いそうなことみたいなのを予測できちゃうんですよねあるいは分析できちゃうと

54
00:04:59,200 --> 00:05:01,860
いうところが悪用されているわけですね

55
00:05:01,860 --> 00:05:04,420
もう一つポルノ

56
00:05:04,420 --> 00:05:08,780
だからエロ動画エロ画像の類ですよね

57
00:05:08,780 --> 00:05:12,880
こういうのは実際に例えば韓国であったんですけれども

58
00:05:12,880 --> 00:05:18,180
実在する女性の顔写真を使って性的な画像を作ると

59
00:05:18,180 --> 00:05:22,380
そういうことがすでにね行われているわけです

60
00:05:22,380 --> 00:05:27,740
なんかまあねどこからか持ってきた裸の画像動画みたいなものに顔をくっつけて

61
00:05:27,740 --> 00:05:33,400
しかもそれが何か本物のように見えるような加工をするっていうディープフェイクですね

62
00:05:33,400 --> 00:05:37,500
まあ本当にあのいろんなケースがあって

63
00:05:37,500 --> 00:05:42,400
女優さんタレント歌手みたいなねすごく有名な人を使うこともあれば

64
00:05:42,400 --> 00:05:45,160
自分のクラスの友達みたいなね

65
00:05:45,160 --> 00:05:51,080
クラスメイトのようなそういう身近な人のそういうディープフェイク画像を作る

66
00:05:51,080 --> 00:05:54,280
あるいは動画を作るそんなケースもね報告をされているところです

67
00:05:54,280 --> 00:05:59,760
これもまあこの人っていうのは学習をさせてですね

68
00:05:59,760 --> 00:06:01,800
大量にデータを読み込ませることによって

69
00:06:01,800 --> 00:06:04,920
まあ大体こんな風な喋り方をするであろうとか

70
00:06:04,920 --> 00:06:11,940
こういうような何か動きをするであろうみたいなことをさせることが

71
00:06:11,940 --> 00:06:15,100
AIってできちゃうんですよね困りますよねこれ犯罪です

72
00:06:15,100 --> 00:06:17,060
こういうことをしちゃいけませんけれども

73
00:06:17,060 --> 00:06:21,800
実際にそういうことが起きてるんで対処していかなきゃいけないよねと

74
00:06:21,800 --> 00:06:26,320
いうところでAI法案というのが作られているわけなんですね

75
00:06:26,320 --> 00:06:28,900
でまああの悪い方ばっかりじゃなくて

76
00:06:28,900 --> 00:06:33,300
AIの研究開発AIの活用こういうのをどんどん進めていきましょう

77
00:06:33,300 --> 00:06:37,960
っていうこともその法律の法案の中には入っているんですけれども

78
00:06:37,960 --> 00:06:42,080
やっぱりどちらかというとやっぱりこういうね悪用を防いでいかなきゃいけない

79
00:06:42,080 --> 00:06:44,260
これ喫緊の課題なわけですよ

80
00:06:44,260 --> 00:06:48,820
ただそれで言うと今回のその日本のAI法案というのは

81
00:06:48,820 --> 00:06:52,600
ん?それ大丈夫なの?っていうところもありまして

82
00:06:52,600 --> 00:06:54,920
罰則がないんですね

83
00:06:54,920 --> 00:07:00,660
法案の中ではAIに対してこの政府に協力しなきゃいけませんと

84
00:07:00,660 --> 00:07:03,960
責務ですっていうねそういう書き方はしてるんですね

85
00:07:03,960 --> 00:07:09,000
だからその日本の政府が考えることにきちんと合わせて

86
00:07:09,000 --> 00:07:13,460
AIっていうのはね使ってください作ってくださいっていうことではあるんですが

87
00:07:13,460 --> 00:07:15,560
そうしなかった場合

88
00:07:15,560 --> 00:07:19,220
例えばだからそうやって選挙中にディープフェイクみたいなのねやって

89
00:07:19,220 --> 00:07:21,880
偽の音声候補者の偽音声を流したりとか

90
00:07:21,880 --> 00:07:27,060
その偽のねエロ画像みたいなのを作ったりっていうことに対して

91
00:07:27,060 --> 00:07:28,260
罰がないんですよね

92
00:07:28,260 --> 00:07:35,220
だから悪い人悪意のある人がいたらそれを止められるかっていうと

93
00:07:35,220 --> 00:07:36,820
ちょっと疑問だねと

94
00:07:36,820 --> 00:07:39,480
ただこれは難しいところもありましてですね

95
00:07:39,480 --> 00:07:43,180
何でもかんでもダメですっていう風に規制してしまう

96
00:07:43,180 --> 00:07:44,800
罰則をつけてしまうと

97
00:07:44,800 --> 00:07:49,540
AIっていうのが本当にこうねこれから進歩をしていくであろうっていう

98
00:07:49,540 --> 00:07:53,260
今段階ですからそういうのを妨げることにもなってしまうかもしれない

99
00:07:53,260 --> 00:07:58,260
あんまり規制ばっかりかけちゃうとね進歩が止まっちゃいますからね

100
00:07:58,260 --> 00:08:01,660
なのでそこは難しいところではあるんですけれども

101
00:08:01,660 --> 00:08:05,980
このAI法案本当に大丈夫なのっていう声もあるということですよね

102
00:08:05,980 --> 00:08:11,400
ちなみにこのAIどんどん進化していますけれども

103
00:08:11,400 --> 00:08:15,540
今後はですね人間並みの知能を持つAI

104
00:08:15,540 --> 00:08:18,600
これはAGIって言いますけれども

105
00:08:18,600 --> 00:08:23,000
今のところは人間が使うっていう形になってますけれども

106
00:08:23,000 --> 00:08:24,540
もう本当に人間と同じくらいだと

107
00:08:24,540 --> 00:08:28,300
さらにはその人類を超えた超知能

108
00:08:28,300 --> 00:08:31,220
知能すっごい頭いいみたいなね

109
00:08:31,220 --> 00:08:33,360
ASIって言いますけれども

110
00:08:33,360 --> 00:08:37,300
このAGIやASIにいずれは行き着くよね

111
00:08:37,300 --> 00:08:41,500
今のところはAIを人間が使うことができているけれども

112
00:08:41,500 --> 00:08:43,060
人類超えてきちゃうよね

113
00:08:43,060 --> 00:08:44,940
そういう見方が多いんですよね

114
00:08:44,940 --> 00:08:48,700
便利な世の中になるとは思いますし

115
00:08:48,700 --> 00:08:51,020
ひょっとするとね私たち働かなくても

116
00:08:51,020 --> 00:08:53,180
AIが勝手に働いてくれて

117
00:08:53,180 --> 00:08:56,840
暮らせるような世の中になるかもしれないんですが

118
00:08:56,840 --> 00:09:00,800
人間がAIを使いこなせなくなる

119
00:09:00,800 --> 00:09:03,600
いうことを聞かせられなくなるっていう

120
00:09:03,600 --> 00:09:05,500
そういう恐れもあるんですよね

121
00:09:05,500 --> 00:09:07,980
これはだからですね

122
00:09:07,980 --> 00:09:10,240
AIに対してはきちんとその法律などでも

123
00:09:10,240 --> 00:09:12,860
対処をしていかなければいけないっていうことが

124
00:09:12,860 --> 00:09:15,200
いろんな人が言われているっていうことなんですね

125
00:09:15,200 --> 00:09:18,080
それでは最後に締めクイズ

126
00:09:18,080 --> 00:09:20,740
番組の最後は番組の内容からクイズを

127
00:09:20,740 --> 00:09:23,580
一問お出ししている次第なんですけれども

128
00:09:23,580 --> 00:09:26,720
今回の話題ですねAI法案だったんですが

129
00:09:26,720 --> 00:09:28,740
その取り締まりもやりますよと

130
00:09:28,740 --> 00:09:33,380
AIの悪用として懸念されているものっていうのが

131
00:09:33,380 --> 00:09:36,440
本物と見分けのつかない偽物

132
00:09:36,440 --> 00:09:39,160
これを作ってしまうことですよと

133
00:09:39,160 --> 00:09:41,800
簡単に作れてしまうことですよっていうことを

134
00:09:41,800 --> 00:09:42,760
申し上げましたね

135
00:09:42,760 --> 00:09:47,040
画像だったり音声だったり動画だったりするわけなんですけれども

136
00:09:47,040 --> 00:09:50,640
この本物と見分けのつかない偽物のことを

137
00:09:50,640 --> 00:09:52,400
何と言うでしょうか

138
00:09:52,400 --> 00:09:55,220
はいちょっとわからないよという方は

139
00:09:55,220 --> 00:09:58,000
もう一度ね番組最初から聞いてみてください

140
00:09:58,000 --> 00:10:00,960
もしくはですねこの番組の概要欄に

141
00:10:00,960 --> 00:10:02,600
記事へのリンク貼っておきますので

142
00:10:02,600 --> 00:10:05,380
そこから理解を深めていただければと思います

143
00:10:05,380 --> 00:10:07,880
というわけで今回の相手は朝日新聞の

144
00:10:07,880 --> 00:10:08,660
神田大輔でした

145
00:10:08,660 --> 00:10:30,360
私たちはきっかけを届ける

146
00:10:30,360 --> 00:10:32,360
人間たちのチームです

147
00:10:32,360 --> 00:10:34,660
新しい朝をつくれ

148
00:10:34,660 --> 00:10:36,240
朝日新聞社

149
00:10:36,240 --> 00:10:38,700
松下光平

150
00:10:38,700 --> 00:10:40,100
三上愛出演

151
00:10:40,100 --> 00:10:41,440
ショートドラマ

152
00:10:41,440 --> 00:10:43,180
新しい朝をつくれ

153
00:10:43,180 --> 00:10:46,080
朝日新聞社ブランドサイトで公開中

